{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.distributions import auto_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.special import expit\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "def split_companies_train_dev_test(companies):\n",
    "    \"Return train, dev, test set for companies\"\n",
    "    train, test = train_test_split(companies, test_size=0.1, stratify = companies.sector)\n",
    "    train, dev = train_test_split(train, test_size=0.1, stratify = train.sector)\n",
    "    return train, dev, test\n",
    "\n",
    "\n",
    "def filter_stocks(stocks, tickers):\n",
    "    return stocks.loc[tickers]\n",
    "\n",
    "\n",
    "def df_to_ts(df):\n",
    "    res = df.copy()\n",
    "    res.index = pd.DatetimeIndex(pd.to_datetime(res.date))\n",
    "    res.drop('date', axis=1)\n",
    "    return res\n",
    "\n",
    "\n",
    "def log_softmax(x):\n",
    "    return x - np.log(np.sum(np.exp(x)))\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return expit(x)\n",
    "\n",
    "\n",
    "def sample_correlation(df, window_size=63):\n",
    "    idx = np.random.randint(0, df.shape[0]-window_size)\n",
    "    ts = df[idx:idx+window_size]\n",
    "    fmap = lambda s: ts['pct_return'].corr(ts[s])\n",
    "    indices = ts.columns.tolist()[1:]\n",
    "    correlations = np.array(list(map(fmap, indices)))\n",
    "    return correlations\n",
    "\n",
    "\n",
    "def create_correlation_score(df, sample_size=1):\n",
    "    res = np.array([log_softmax(sample_correlation(df)/0.05)\n",
    "                    for i in range(sample_size)])\n",
    "    return np.exp(np.nanmean(res, 0))\n",
    "\n",
    "\n",
    "def load_data(stock_filename=None, indices_filename=None):\n",
    "\n",
    "    if stock_filename is None:\n",
    "        stock_filename = '../../data/processed/wiki_stocks_returns.csv'\n",
    "\n",
    "    if indices_filename is None:\n",
    "        indices_filename = '../../data/processed/wiki_indices_returns.csv'\n",
    "\n",
    "    stocks = pd.read_csv(stock_filename, index_col=False) # long format\n",
    "    indices = pd.read_csv(indices_filename, index_col=False) # wide format\n",
    "\n",
    "    # Implementation of hierarchical clustering\n",
    "    drop_column = lambda df,i=0: df.drop(df.columns[i], axis=1)\n",
    "\n",
    "    stocks = drop_column(stocks)\n",
    "    stocks = stocks.drop('name', axis=1)\n",
    "    stocks = stocks.dropna()\n",
    "\n",
    "    companies = stocks.groupby('ticker').first().reset_index()\n",
    "    sectors_counts = companies.sector.value_counts()\n",
    "    sectors_proportions = sectors_counts/sectors_counts.sum()\n",
    "    sectors_unique = sectors_counts.index.tolist()\n",
    "\n",
    "    stocks = stocks.set_index('ticker')\n",
    "\n",
    "    indices_ts = df_to_ts(indices[['date'] + sectors_unique])\n",
    "    stocks_ts = df_to_ts(stocks.reset_index())\n",
    "\n",
    "    stocks_all = pd.merge(stocks_ts, indices_ts, 'left')\n",
    "    stocks_all = stocks_all.dropna() # loss of 200 000 observations\n",
    "    stocks_all = stocks_all.drop('sector', axis=1)\n",
    "    stocks_all = stocks_all.groupby('ticker').apply(df_to_ts)\n",
    "    stocks_all = stocks_all.drop(['ticker', 'date'], axis=1)\n",
    "    stocks_all = stocks_all.rename(columns={'close': 'pct_return'})\n",
    "\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    label_encoder.fit(sectors_counts.index.tolist())\n",
    "    ticker_to_sector = dict(zip(companies.ticker, label_encoder.transform(companies.sector)))\n",
    "\n",
    "    return stocks_all, companies, label_encoder, ticker_to_sector\n",
    "\n",
    "def sectors_statistics(companies):\n",
    "    sectors_counts = companies.sector.value_counts()\n",
    "    sectors_proportions = sectors_counts/sectors_counts.sum()\n",
    "    sectors_unique = sectors_counts.index.tolist()\n",
    "    return sectors_counts, sectors_proportions, sectors_unique\n",
    "\n",
    "\n",
    "def add_common_layers(y):\n",
    "    y = keras.layers.BatchNormalization()(y)\n",
    "    y = keras.layers.LeakyReLU()(y)\n",
    "    return y\n",
    "\n",
    "\n",
    "def grouped_convolution(y, nb_channels, _strides, cardinality=4):\n",
    "    # when `cardinality` == 1 this is just a standard convolution\n",
    "    if cardinality == 1:\n",
    "        return keras.layers.Conv1D(nb_channels, kernel_size=10, strides=_strides, padding='same')(y)\n",
    "\n",
    "    assert not nb_channels % cardinality\n",
    "    _d = nb_channels // cardinality\n",
    "\n",
    "    # in a grouped convolution layer, input and output channels are divided into `cardinality` groups,\n",
    "    # and convolutions are separately performed within each group\n",
    "    groups = []\n",
    "    for j in range(cardinality):\n",
    "        group = keras.layers.Lambda(lambda z: z[:, :, j * _d:j * _d + _d])(y)\n",
    "        groups.append(keras.layers.Conv1D(_d, kernel_size=10, strides=_strides, padding='same')(group))\n",
    "\n",
    "    # the grouped convolutional layer concatenates them as the outputs of the layer\n",
    "    y = keras.layers.concatenate(groups)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def residual_block(y, nb_channels_in, nb_channels_out, cardinality=4, _strides=1, _project_shortcut=False):\n",
    "    \"\"\"\n",
    "    Our network consists of a stack of residual blocks. These blocks have the same topology,\n",
    "    and are subject to two simple rules:\n",
    "    - If producing spatial maps of the same size, the blocks share the same hyper-parameters (width and filter sizes).\n",
    "    - Each time the spatial map is down-sampled by a factor of 2, the width of the blocks is multiplied by a factor of 2.\n",
    "    \"\"\"\n",
    "    shortcut = y\n",
    "    kl = keras.layers\n",
    "    # we modify the residual building block as a bottleneck design to make the network more economical\n",
    "    y = kl.Conv1D(nb_channels_in, kernel_size=1, strides=1, padding='same')(y)\n",
    "    y = add_common_layers(y)\n",
    "\n",
    "    # ResNeXt (identical to ResNet when `cardinality` == 1)\n",
    "    y = grouped_convolution(y, nb_channels_in, _strides=_strides)\n",
    "    y = add_common_layers(y)\n",
    "\n",
    "    y = kl.Conv1D(nb_channels_out, kernel_size=1, strides=1, padding='same')(y)\n",
    "    # batch normalization is employed after aggregating the transformations and before adding to the shortcut\n",
    "    y = kl.BatchNormalization()(y)\n",
    "\n",
    "    # identity shortcuts used directly when the input and output are of the same dimensions\n",
    "    if _project_shortcut or _strides != 1:\n",
    "        # when the dimensions increase projection shortcut is used to match dimensions (done by 1Ã—1 convolutions)\n",
    "        # when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2\n",
    "        shortcut = kl.Conv1D(nb_channels_out, kernel_size=1, strides=_strides, padding='same')(shortcut)\n",
    "        shortcut = kl.BatchNormalization()(shortcut)\n",
    "\n",
    "    y = kl.add([shortcut, y])\n",
    "\n",
    "    # relu is performed right after each batch normalization,\n",
    "    # expect for the output of the block where relu is performed after the adding to the shortcut\n",
    "    y = kl.LeakyReLU()(y)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "# reparameterization trick\n",
    "# instead of sampling from Q(z|X), sample eps = N(0,I)\n",
    "# z = z_mean + sqrt(var)*eps\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "    # Arguments:\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns:\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "    z_mean, z_log_var = args\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    dim = keras.backend.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = tf.random_normal(shape=(batch, dim))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "class CovarianceLayer(keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        self.num_classes = num_classes\n",
    "        super(CovarianceLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        super(CovarianceLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs):\n",
    "        series_input, environment_input = inputs\n",
    "        series_input_multiple = tf.tile(series_input, [1, 1, self.num_classes])\n",
    "        covariances = tf.reduce_mean(series_input_multiple * environment_input, axis=1)\n",
    "        \n",
    "        return covariances\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "\n",
    "def random_subset(df, window_size=21):\n",
    "    idx = np.random.randint(0, df.shape[0]-window_size)\n",
    "    ts = df[idx:idx+window_size]\n",
    "    return ts\n",
    "\n",
    "\n",
    "def make_keras_subset(dataset_type, companies_data, stocks_data, label_encoder, batch_size, window_size=21):\n",
    "    idx = np.random.choice(companies_data[dataset_type].shape[0], batch_size)\n",
    "    df = companies_data[dataset_type].iloc[idx]\n",
    "\n",
    "    model_input_data = [random_subset(stocks_data[dataset_type].loc[t], window_size) for t in df.ticker]\n",
    "    model_series_input = np.array([df['pct_return'].values for df in model_input_data])\n",
    "    model_series_input = model_series_input.reshape(-1, window_size, 1)\n",
    "\n",
    "    model_environment_input = np.array([df.iloc[:, 1:].values for df in model_input_data])\n",
    "\n",
    "    y_true = label_encoder.transform(df.sector)\n",
    "\n",
    "    return model_series_input, model_environment_input, y_true\n",
    "\n",
    "\n",
    "class StocksSequence(keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, stocks_data,  companies_data, window_size, label_encoder, batch_size):\n",
    "        self.stocks_data = stocks_data\n",
    "        self.batch_size = batch_size\n",
    "        self.label_encoder = label_encoder\n",
    "        self.companies_data = companies_data\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        _, sectors_proportion, _ = sectors_statistics(companies_data)\n",
    "        self.sectors_proportion = sectors_proportion\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.stocks_data.shape[0] / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        idx = np.random.choice(self.companies_data.shape[0], self.batch_size)\n",
    "        df = self.companies_data.iloc[idx]\n",
    "        model_input_data = [random_subset(self.stocks_data.loc[t], self.window_size) for t in df.ticker]\n",
    "        model_input = np.array([df.values for df in model_input_data])\n",
    "        # model_input = np.transpose(model_input, [0, 2, 1])\n",
    "        y_true = self.label_encoder.transform(df.sector)\n",
    "        return model_input, y_true\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_covariance_convolution_weight(shape, dtype=None):\n",
    "    kernel_shape = shape[0]\n",
    "    num_classes = shape[1]\n",
    "    filters = shape[2]\n",
    "    x = np.zeros([kernel_shape, num_classes, filters])\n",
    "    noise = np.random.normal(0, 0.001, size=[kernel_shape, num_classes, filters])\n",
    "\n",
    "    for j in range(0, filters):\n",
    "        j_class = j % num_classes\n",
    "        x[:, [0, j_class], j] = 1\n",
    "    return x + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(num_classes=16, window_size=21, latent_dim=32):\n",
    "    kl = keras.layers\n",
    "    \n",
    "    model_input = keras.layers.Input(shape=(window_size, num_classes+1), dtype='float32', name='series_input')\n",
    "    x_gaussian = kl.GaussianNoise(0.0001)(model_input)\n",
    "    tanh_layer = kl.Lambda(lambda y: tf.tanh(tf.scalar_mul(100, y))) \n",
    "    x = tanh_layer(x_gaussian)   \n",
    "    x_skip = kl.Conv1D(64, 1, 1, activation='relu', \n",
    "                   kernel_initializer=keras.initializers.Orthogonal())(x)\n",
    "    \n",
    "    x = kl.BatchNormalization()(x_skip)\n",
    "    x = kl.Conv1D(64, 3, 2, activation='relu',\n",
    "                  kernel_initializer=keras.initializers.Orthogonal(),\n",
    "                  kernel_regularizer=keras.regularizers.l1_l2(l1=0.00001, l2=0.00001))(x)\n",
    "    x = kl.BatchNormalization()(x)\n",
    "    x = kl.Conv1D(128, 3, 1, activation='relu',\n",
    "                  kernel_initializer=keras.initializers.Orthogonal())(x)\n",
    "    x = kl.BatchNormalization()(x)\n",
    "    x = kl.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    y = kl.GlobalAveragePooling1D()(x_skip)\n",
    "    \n",
    "    x_gaussian_scaled = kl.BatchNormalization(center=False)(x_gaussian)\n",
    "    z = kl.Conv1D(64, 1, 1, activation='relu', \n",
    "                  kernel_initializer=keras.initializers.Orthogonal(),\n",
    "                  kernel_regularizer=keras.regularizers.l1_l2(l1=0.0001, l2=0.0001))(x_gaussian_scaled)\n",
    "    z = kl.BatchNormalization()(z)\n",
    "    z = kl.Conv1D(128, 3, 2, activation='relu', \n",
    "                  kernel_initializer=keras.initializers.Orthogonal(),\n",
    "                  kernel_regularizer=keras.regularizers.l1_l2(l1=0.0001, l2=0.0001))(z)\n",
    "    z = kl.BatchNormalization()(z)\n",
    "    z = kl.GlobalAveragePooling1D()(z)\n",
    "    \n",
    "    x = kl.Concatenate()([x, y, z])\n",
    "    z = kl.BatchNormalization()(z)\n",
    "    x = kl.Dense(128, 'relu', name='Embedding', \n",
    "                 kernel_regularizer=keras.regularizers.l1_l2(l1=0.00001, l2=0.00001))(x)\n",
    "    x = kl.GaussianNoise(0.0001)(x)\n",
    "    x_pred = kl.Dense(num_classes, 'softmax', \n",
    "                      kernel_regularizer=keras.regularizers.l1_l2(l1=0.00001, l2=0.00001))(x)\n",
    "\n",
    "    model = keras.Model(inputs = model_input, outputs=[x_pred], name='Classifier')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most representated class: Financial Services , with proportion of  13.09 %.\n"
     ]
    }
   ],
   "source": [
    "# Make train dev test set.\n",
    "np.random.seed(42)\n",
    "\n",
    "### Feature engineering\n",
    "\n",
    "stock_filename = '../data/processed/wiki_stocks_returns.csv'\n",
    "indices_filename = '../data/processed/wiki_indices_returns.csv'\n",
    "\n",
    "stocks_all, companies, label_encoder, ticker_to_sector = load_data(stock_filename, indices_filename)\n",
    "sectors_counts, sectors_proportions, sectors_unique = sectors_statistics(companies)\n",
    "\n",
    "max_proportion_baseline = sectors_proportions.max()\n",
    "biggest_sector = sectors_proportions.argmax()\n",
    "\n",
    "print(\"Most representated class:\", biggest_sector, ', with proportion of ', round(100*max_proportion_baseline, 2), '%.')\n",
    "# Accuracy of our models should be better than max_proportion_baseline.\n",
    "\n",
    "companies_data = {}\n",
    "data_split = split_companies_train_dev_test(companies)\n",
    "for i, k in enumerate(['train', 'dev', 'test']):\n",
    "    companies_data[k] = data_split[i]\n",
    "stocks_data = {k: filter_stocks(stocks_all, v.ticker) for k, v in companies_data.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 63\n",
    "model = make_model(window_size=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer #1 (named \"batch_normalization_86\" in the current model) was found to correspond to layer batch_normalization_79 in the save file. However the new layer batch_normalization_86 expects 3 weights, but the saved weights have 4 elements.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-db2e5796ab70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoint/model_weights_twentieth.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m   1446\u001b[0m         \u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_post_build_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    798\u001b[0m                        \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m                        \u001b[0;34m' weights, but the saved weights have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m                        str(len(weight_values)) + ' elements.')\n\u001b[0m\u001b[1;32m    801\u001b[0m     \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m   \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Layer #1 (named \"batch_normalization_86\" in the current model) was found to correspond to layer batch_normalization_79 in the save file. However the new layer batch_normalization_86 expects 3 weights, but the saved weights have 4 elements."
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    model.load_weights('checkpoint/model_weights_twentieth.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def schedule_fn(epoch):\n",
    "   initial_lrate = 0.001\n",
    "   drop = 0.9\n",
    "   epochs_drop = 10.0\n",
    "   epoch = epoch\n",
    "   pow_drop = (1+epoch)/epochs_drop\n",
    "   lrate = initial_lrate * math.pow(drop,  \n",
    "           math.floor(pow_drop))\n",
    "   return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "series_input (InputLayer)       (None, 63, 17)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_48 (GaussianNois (None, 63, 17)       0           series_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)              (None, 63, 17)       0           gaussian_noise_48[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_84 (Conv1D)              (None, 63, 64)       1152        lambda_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, 63, 64)       256         conv1d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, 63, 17)       51          gaussian_noise_48[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_85 (Conv1D)              (None, 31, 64)       12352       batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_87 (Conv1D)              (None, 63, 64)       1152        batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, 31, 64)       256         conv1d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, 63, 64)       256         conv1d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_86 (Conv1D)              (None, 29, 128)      24704       batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_88 (Conv1D)              (None, 31, 128)      24704       batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, 29, 128)      512         conv1d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, 31, 128)      512         conv1d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_50 (Gl (None, 128)          0           batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_51 (Gl (None, 64)           0           conv1d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_52 (Gl (None, 128)          0           batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 320)          0           global_average_pooling1d_50[0][0]\n",
      "                                                                 global_average_pooling1d_51[0][0]\n",
      "                                                                 global_average_pooling1d_52[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Embedding (Dense)               (None, 128)          41088       concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_49 (GaussianNois (None, 128)          0           Embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 16)           2064        gaussian_noise_49[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 109,059\n",
      "Trainable params: 108,129\n",
      "Non-trainable params: 930\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "\n",
    "optimizer = keras.optimizers.Adam(0.001)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint('checkpoint/model_weights_twenty_first.json', monitor='val_acc', verbose=1, save_best_only=True, mode='max'),\n",
    "    keras.callbacks.TensorBoard(log_dir='./logs/twenty_first'),\n",
    "    keras.callbacks.LearningRateScheduler(schedule_fn)\n",
    "]\n",
    "\n",
    "stocks_sequence_training = StocksSequence(stocks_data['train'], companies_data['train'], window_size, label_encoder, batch_size)\n",
    "stocks_sequence_validation = StocksSequence(stocks_data['dev'], companies_data['dev'], window_size, label_encoder, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4140:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 862, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/utils/data_utils.py\", line 577, in _run\n",
      "    with closing(self.executor_fn(_SHARED_SEQUENCES)) as executor:\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/utils/data_utils.py\", line 551, in <lambda>\n",
      "    workers, initializer=init_pool, initargs=(seqs,))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/context.py\", line 118, in Pool\n",
      "    context=self.get_context())\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 168, in __init__\n",
      "    self._repopulate_pool()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 233, in _repopulate_pool\n",
      "    w.start()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 105, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/context.py\", line 267, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/popen_fork.py\", line 20, in __init__\n",
      "    self._launch(process_obj)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/popen_fork.py\", line 67, in _launch\n",
      "    self.pid = os.fork()\n",
      "OSError: [Errno 12] Cannot allocate memory\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-6e9389b9a5d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mstocks_sequence_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstocks_sequence_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     use_multiprocessing=True, initial_epoch=0)\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mstocks_sequence_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStocksSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstocks_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompanies_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1777\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1778\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1779\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1781\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    173\u001b[0m       \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    stocks_sequence_training, steps_per_epoch=500, epochs=400, callbacks=callbacks, \n",
    "    validation_data = stocks_sequence_validation, validation_steps=50, workers=4, max_queue_size=20, verbose=1, \n",
    "    use_multiprocessing=True, initial_epoch=0)\n",
    "\n",
    "stocks_sequence_test = StocksSequence(stocks_data['test'], companies_data['test'], window_size, label_encoder, batch_size)\n",
    "\n",
    "model.evaluate_generator(stocks_sequence_test, 400, workers=2, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "name": "02-dph-model-training.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
