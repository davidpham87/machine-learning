{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.distributions import auto_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.special import expit\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "def split_companies_train_dev_test(companies):\n",
    "    \"Return train, dev, test set for companies\"\n",
    "    train, test = train_test_split(companies, test_size=0.1, stratify = companies.sector)\n",
    "    train, dev = train_test_split(train, test_size=0.1, stratify = train.sector)\n",
    "    return train, dev, test\n",
    "\n",
    "\n",
    "def filter_stocks(stocks, tickers):\n",
    "    return stocks.loc[tickers]\n",
    "\n",
    "\n",
    "def df_to_ts(df):\n",
    "    res = df.copy()\n",
    "    res.index = pd.DatetimeIndex(pd.to_datetime(res.date))\n",
    "    res.drop('date', axis=1)\n",
    "    return res\n",
    "\n",
    "\n",
    "def log_softmax(x):\n",
    "    return x - np.log(np.sum(np.exp(x)))\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return expit(x)\n",
    "\n",
    "\n",
    "def sample_correlation(df, window_size=63):\n",
    "    idx = np.random.randint(0, df.shape[0]-window_size)\n",
    "    ts = df[idx:idx+window_size]\n",
    "    fmap = lambda s: ts['pct_return'].corr(ts[s])\n",
    "    indices = ts.columns.tolist()[1:]\n",
    "    correlations = np.array(list(map(fmap, indices)))\n",
    "    return correlations\n",
    "\n",
    "\n",
    "def create_correlation_score(df, sample_size=1):\n",
    "    res = np.array([log_softmax(sample_correlation(df)/0.05)\n",
    "                    for i in range(sample_size)])\n",
    "    return np.exp(np.nanmean(res, 0))\n",
    "\n",
    "\n",
    "def load_data(stock_filename=None, indices_filename=None):\n",
    "\n",
    "    if stock_filename is None:\n",
    "        stock_filename = '../../data/processed/wiki_stocks_returns.csv'\n",
    "\n",
    "    if indices_filename is None:\n",
    "        indices_filename = '../../data/processed/wiki_indices_returns.csv'\n",
    "\n",
    "    stocks = pd.read_csv(stock_filename, index_col=False) # long format\n",
    "    indices = pd.read_csv(indices_filename, index_col=False) # wide format\n",
    "\n",
    "    # Implementation of hierarchical clustering\n",
    "    drop_column = lambda df,i=0: df.drop(df.columns[i], axis=1)\n",
    "\n",
    "    stocks = drop_column(stocks)\n",
    "    stocks = stocks.drop('name', axis=1)\n",
    "    stocks = stocks.dropna()\n",
    "\n",
    "    companies = stocks.groupby('ticker').first().reset_index()\n",
    "    sectors_counts = companies.sector.value_counts()\n",
    "    sectors_proportions = sectors_counts/sectors_counts.sum()\n",
    "    sectors_unique = sectors_counts.index.tolist()\n",
    "\n",
    "    stocks = stocks.set_index('ticker')\n",
    "\n",
    "    indices_ts = df_to_ts(indices[['date'] + sectors_unique])\n",
    "    stocks_ts = df_to_ts(stocks.reset_index())\n",
    "\n",
    "    stocks_all = pd.merge(stocks_ts, indices_ts, 'left')\n",
    "    stocks_all = stocks_all.dropna() # loss of 200 000 observations\n",
    "    stocks_all = stocks_all.drop('sector', axis=1)\n",
    "    stocks_all = stocks_all.groupby('ticker').apply(df_to_ts)\n",
    "    stocks_all = stocks_all.drop(['ticker', 'date'], axis=1)\n",
    "    stocks_all = stocks_all.rename(columns={'close': 'pct_return'})\n",
    "\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    label_encoder.fit(sectors_counts.index.tolist())\n",
    "    ticker_to_sector = dict(zip(companies.ticker, label_encoder.transform(companies.sector)))\n",
    "\n",
    "    return stocks_all, companies, label_encoder, ticker_to_sector\n",
    "\n",
    "def sectors_statistics(companies):\n",
    "    sectors_counts = companies.sector.value_counts()\n",
    "    sectors_proportions = sectors_counts/sectors_counts.sum()\n",
    "    sectors_unique = sectors_counts.index.tolist()\n",
    "    return sectors_counts, sectors_proportions, sectors_unique\n",
    "\n",
    "\n",
    "def add_common_layers(y):\n",
    "    y = keras.layers.BatchNormalization()(y)\n",
    "    y = keras.layers.LeakyReLU()(y)\n",
    "    return y\n",
    "\n",
    "\n",
    "def grouped_convolution(y, nb_channels, _strides, cardinality=4):\n",
    "    # when `cardinality` == 1 this is just a standard convolution\n",
    "    if cardinality == 1:\n",
    "        return keras.layers.Conv1D(nb_channels, kernel_size=10, strides=_strides, padding='same')(y)\n",
    "\n",
    "    assert not nb_channels % cardinality\n",
    "    _d = nb_channels // cardinality\n",
    "\n",
    "    # in a grouped convolution layer, input and output channels are divided into `cardinality` groups,\n",
    "    # and convolutions are separately performed within each group\n",
    "    groups = []\n",
    "    for j in range(cardinality):\n",
    "        group = keras.layers.Lambda(lambda z: z[:, :, j * _d:j * _d + _d])(y)\n",
    "        groups.append(keras.layers.Conv1D(_d, kernel_size=10, strides=_strides, padding='same')(group))\n",
    "\n",
    "    # the grouped convolutional layer concatenates them as the outputs of the layer\n",
    "    y = keras.layers.concatenate(groups)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def residual_block(y, nb_channels_in, nb_channels_out, cardinality=4, _strides=1, _project_shortcut=False):\n",
    "    \"\"\"\n",
    "    Our network consists of a stack of residual blocks. These blocks have the same topology,\n",
    "    and are subject to two simple rules:\n",
    "    - If producing spatial maps of the same size, the blocks share the same hyper-parameters (width and filter sizes).\n",
    "    - Each time the spatial map is down-sampled by a factor of 2, the width of the blocks is multiplied by a factor of 2.\n",
    "    \"\"\"\n",
    "    shortcut = y\n",
    "    kl = keras.layers\n",
    "    # we modify the residual building block as a bottleneck design to make the network more economical\n",
    "    y = kl.Conv1D(nb_channels_in, kernel_size=1, strides=1, padding='same')(y)\n",
    "    y = add_common_layers(y)\n",
    "\n",
    "    # ResNeXt (identical to ResNet when `cardinality` == 1)\n",
    "    y = grouped_convolution(y, nb_channels_in, _strides=_strides)\n",
    "    y = add_common_layers(y)\n",
    "\n",
    "    y = kl.Conv1D(nb_channels_out, kernel_size=1, strides=1, padding='same')(y)\n",
    "    # batch normalization is employed after aggregating the transformations and before adding to the shortcut\n",
    "    y = kl.BatchNormalization()(y)\n",
    "\n",
    "    # identity shortcuts used directly when the input and output are of the same dimensions\n",
    "    if _project_shortcut or _strides != 1:\n",
    "        # when the dimensions increase projection shortcut is used to match dimensions (done by 1Ã—1 convolutions)\n",
    "        # when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2\n",
    "        shortcut = kl.Conv1D(nb_channels_out, kernel_size=1, strides=_strides, padding='same')(shortcut)\n",
    "        shortcut = kl.BatchNormalization()(shortcut)\n",
    "\n",
    "    y = kl.add([shortcut, y])\n",
    "\n",
    "    # relu is performed right after each batch normalization,\n",
    "    # expect for the output of the block where relu is performed after the adding to the shortcut\n",
    "    y = kl.LeakyReLU()(y)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "# reparameterization trick\n",
    "# instead of sampling from Q(z|X), sample eps = N(0,I)\n",
    "# z = z_mean + sqrt(var)*eps\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "    # Arguments:\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns:\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "    z_mean, z_log_var = args\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    dim = keras.backend.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = tf.random_normal(shape=(batch, dim))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "class CovarianceLayer(keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        self.num_classes = num_classes\n",
    "        super(CovarianceLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        super(CovarianceLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs):\n",
    "        series_input, environment_input = inputs\n",
    "        series_input_multiple = tf.tile(series_input, [1, 1, self.num_classes])\n",
    "        covariances = tf.reduce_mean(series_input_multiple * environment_input, axis=1)\n",
    "        \n",
    "        return covariances\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "\n",
    "def random_subset(df, window_size=21):\n",
    "    idx = np.random.randint(0, df.shape[0]-window_size)\n",
    "    ts = df[idx:idx+window_size]\n",
    "    return ts\n",
    "\n",
    "\n",
    "def make_keras_subset(dataset_type, companies_data, stocks_data, label_encoder, batch_size, window_size=21):\n",
    "    idx = np.random.choice(companies_data[dataset_type].shape[0], batch_size)\n",
    "    df = companies_data[dataset_type].iloc[idx]\n",
    "\n",
    "    model_input_data = [random_subset(stocks_data[dataset_type].loc[t], window_size) for t in df.ticker]\n",
    "    model_series_input = np.array([df['pct_return'].values for df in model_input_data])\n",
    "    model_series_input = model_series_input.reshape(-1, window_size, 1)\n",
    "\n",
    "    model_environment_input = np.array([df.iloc[:, 1:].values for df in model_input_data])\n",
    "\n",
    "    y_true = label_encoder.transform(df.sector)\n",
    "\n",
    "    return model_series_input, model_environment_input, y_true\n",
    "\n",
    "\n",
    "class StocksSequence(keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, stocks_data,  companies_data, window_size, label_encoder, batch_size):\n",
    "        self.stocks_data = stocks_data\n",
    "        self.batch_size = batch_size\n",
    "        self.label_encoder = label_encoder\n",
    "        self.companies_data = companies_data\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        _, sectors_proportion, _ = sectors_statistics(companies_data)\n",
    "        self.sectors_proportion = sectors_proportion\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.stocks_data.shape[0] / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        idx = np.random.choice(self.companies_data.shape[0], self.batch_size)\n",
    "        df = self.companies_data.iloc[idx]\n",
    "        model_input_data = [random_subset(self.stocks_data.loc[t], self.window_size) for t in df.ticker]\n",
    "        model_input = np.array([df.values for df in model_input_data])\n",
    "        # model_input = np.transpose(model_input, [0, 2, 1])\n",
    "        y_true = self.label_encoder.transform(df.sector)\n",
    "        return model_input, y_true\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_covariance_convolution_weight(shape, dtype=None):\n",
    "    kernel_shape = shape[0]\n",
    "    num_classes = shape[1]\n",
    "    filters = shape[2]\n",
    "    x = np.zeros([kernel_shape, num_classes, filters])\n",
    "    noise = np.random.normal(0, 0.001, size=[kernel_shape, num_classes, filters])\n",
    "\n",
    "    for j in range(0, filters):\n",
    "        j_class = j % num_classes\n",
    "        x[:, [0, j_class], j] = 1\n",
    "    return x + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(num_classes=16, window_size=21, latent_dim=32):\n",
    "    kl = keras.layers\n",
    "    \n",
    "    model_input = keras.layers.Input(shape=(window_size, num_classes+1), dtype='float32', name='series_input')\n",
    "    x_gaussian = kl.GaussianNoise(0.0002)(model_input)\n",
    "    tanh_layer = kl.Lambda(lambda y: tf.tanh(tf.scalar_mul(100, y))) \n",
    "    x = tanh_layer(x_gaussian)   \n",
    "    x_skip = kl.Conv1D(64, 1, 1, activation='relu', \n",
    "                   kernel_initializer=keras.initializers.Orthogonal())(x)\n",
    "    \n",
    "    x = kl.BatchNormalization()(x_skip)\n",
    "    x = kl.Conv1D(64, 3, 2, activation='relu',\n",
    "                  kernel_initializer=keras.initializers.Orthogonal(),\n",
    "                  kernel_regularizer=keras.regularizers.l1_l2(l1=0.0001, l2=0.0001))(x)\n",
    "    x = kl.BatchNormalization()(x)\n",
    "    x = kl.Conv1D(128, 3, 1, activation='relu',\n",
    "                  kernel_initializer=keras.initializers.Orthogonal())(x)\n",
    "    x = kl.BatchNormalization()(x)\n",
    "    x = kl.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    y = kl.GlobalAveragePooling1D()(x_skip)\n",
    "    \n",
    "    x_gaussian_scaled = kl.BatchNormalization(center=False)(x_gaussian)\n",
    "    z = kl.Conv1D(64, 1, 1, activation='relu', \n",
    "                  kernel_initializer=keras.initializers.Orthogonal(),\n",
    "                  kernel_regularizer=keras.regularizers.l1_l2(l1=0.001, l2=0.001))(x_gaussian_scaled)\n",
    "    z = kl.BatchNormalization()(z)\n",
    "    z = kl.Conv1D(128, 3, 2, activation='relu', \n",
    "                  kernel_initializer=keras.initializers.Orthogonal(),\n",
    "                  kernel_regularizer=keras.regularizers.l1_l2(l1=0.001, l2=0.001))(z)\n",
    "    z = kl.BatchNormalization()(z)\n",
    "    z = kl.GlobalAveragePooling1D()(z)\n",
    "    \n",
    "    x = kl.Concatenate()([x, y, z])\n",
    "    x = kl.GaussianNoise(0.01)(x)\n",
    "    z = kl.BatchNormalization()(z)\n",
    "    x = kl.Dense(128, 'relu', name='Embedding', \n",
    "                 kernel_regularizer=keras.regularizers.l1_l2(l1=0.00001, l2=0.00001))(x)\n",
    "    x = kl.GaussianNoise(0.01)(x)\n",
    "    x_pred = kl.Dense(num_classes, 'softmax', \n",
    "                      kernel_regularizer=keras.regularizers.l1_l2(l1=0.00001, l2=0.00001))(x)\n",
    "\n",
    "    model = keras.Model(inputs = model_input, outputs=[x_pred], name='Classifier')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-7365f607241b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mindices_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../data/processed/wiki_indices_returns.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mstocks_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompanies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticker_to_sector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0msectors_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msectors_proportions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msectors_unique\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msectors_statistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompanies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-80c86c0fc96b>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(stock_filename, indices_filename)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mindices_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../../data/processed/wiki_indices_returns.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mstocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# long format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# wide format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1005\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'as_recarray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1748\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1749\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read (pandas/_libs/parsers.c:10862)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory (pandas/_libs/parsers.c:11138)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows (pandas/_libs/parsers.c:12175)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data (pandas/_libs/parsers.c:14136)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens (pandas/_libs/parsers.c:14858)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype (pandas/_libs/parsers.c:15629)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_integer_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_integer_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m     \"\"\"\n\u001b[1;32m    742\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mprovided\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0man\u001b[0m \u001b[0minteger\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Make train dev test set.\n",
    "np.random.seed(42)\n",
    "\n",
    "### Feature engineering\n",
    "\n",
    "stock_filename = '../data/processed/wiki_stocks_returns.csv'\n",
    "indices_filename = '../data/processed/wiki_indices_returns.csv'\n",
    "\n",
    "stocks_all, companies, label_encoder, ticker_to_sector = load_data(stock_filename, indices_filename)\n",
    "sectors_counts, sectors_proportions, sectors_unique = sectors_statistics(companies)\n",
    "\n",
    "max_proportion_baseline = sectors_proportions.max()\n",
    "biggest_sector = sectors_proportions.argmax()\n",
    "\n",
    "print(\"Most representated class:\", biggest_sector, ', with proportion of ', round(100*max_proportion_baseline, 2), '%.')\n",
    "# Accuracy of our models should be better than max_proportion_baseline.\n",
    "\n",
    "companies_data = {}\n",
    "data_split = split_companies_train_dev_test(companies)\n",
    "for i, k in enumerate(['train', 'dev', 'test']):\n",
    "    companies_data[k] = data_split[i]\n",
    "stocks_data = {k: filter_stocks(stocks_all, v.ticker) for k, v in companies_data.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 63\n",
    "model = make_model(window_size=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer #1 (named \"batch_normalization_86\" in the current model) was found to correspond to layer batch_normalization_79 in the save file. However the new layer batch_normalization_86 expects 3 weights, but the saved weights have 4 elements.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-db2e5796ab70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoint/model_weights_twentieth.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m   1446\u001b[0m         \u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_post_build_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    798\u001b[0m                        \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m                        \u001b[0;34m' weights, but the saved weights have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m                        str(len(weight_values)) + ' elements.')\n\u001b[0m\u001b[1;32m    801\u001b[0m     \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m   \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Layer #1 (named \"batch_normalization_86\" in the current model) was found to correspond to layer batch_normalization_79 in the save file. However the new layer batch_normalization_86 expects 3 weights, but the saved weights have 4 elements."
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    model.load_weights('checkpoint/model_weights_twentieth.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def schedule_fn(epoch):\n",
    "   initial_lrate = 0.001\n",
    "   drop = 0.9\n",
    "   epochs_drop = 10.0\n",
    "   epoch = epoch\n",
    "   pow_drop = (1+epoch)/epochs_drop\n",
    "   lrate = initial_lrate * math.pow(drop,  \n",
    "           math.floor(pow_drop))\n",
    "   return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "series_input (InputLayer)       (None, 63, 17)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_45 (GaussianNois (None, 63, 17)       0           series_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 63, 17)       0           gaussian_noise_45[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_79 (Conv1D)              (None, 63, 64)       1152        lambda_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 63, 64)       256         conv1d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 63, 17)       51          gaussian_noise_45[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_80 (Conv1D)              (None, 31, 64)       12352       batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_82 (Conv1D)              (None, 63, 64)       1152        batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 31, 64)       256         conv1d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 63, 64)       256         conv1d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_81 (Conv1D)              (None, 29, 128)      24704       batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_83 (Conv1D)              (None, 31, 128)      24704       batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 29, 128)      512         conv1d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 31, 128)      512         conv1d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_47 (Gl (None, 128)          0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_48 (Gl (None, 64)           0           conv1d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_49 (Gl (None, 128)          0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 320)          0           global_average_pooling1d_47[0][0]\n",
      "                                                                 global_average_pooling1d_48[0][0]\n",
      "                                                                 global_average_pooling1d_49[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_46 (GaussianNois (None, 320)          0           concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Embedding (Dense)               (None, 128)          41088       gaussian_noise_46[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_47 (GaussianNois (None, 128)          0           Embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 16)           2064        gaussian_noise_47[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 109,059\n",
      "Trainable params: 108,129\n",
      "Non-trainable params: 930\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "\n",
    "optimizer = keras.optimizers.Adam(0.001)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint('checkpoint/model_weights_twenty_first.json', monitor='val_acc', verbose=1, save_best_only=True, mode='max'),\n",
    "    keras.callbacks.TensorBoard(log_dir='./logs/twenty_first'),\n",
    "    keras.callbacks.LearningRateScheduler(schedule_fn)\n",
    "]\n",
    "\n",
    "stocks_sequence_training = StocksSequence(stocks_data['train'], companies_data['train'], window_size, label_encoder, batch_size)\n",
    "stocks_sequence_validation = StocksSequence(stocks_data['dev'], companies_data['dev'], window_size, label_encoder, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(\n",
    "    stocks_sequence_training, steps_per_epoch=500, epochs=400, callbacks=callbacks, \n",
    "    validation_data = stocks_sequence_validation, validation_steps=50, workers=4, max_queue_size=20, verbose=1, \n",
    "    use_multiprocessing=True, initial_epoch=0)\n",
    "\n",
    "stocks_sequence_test = StocksSequence(stocks_data['test'], companies_data['test'], window_size, label_encoder, batch_size)\n",
    "\n",
    "model.evaluate_generator(stocks_sequence_test, 400, workers=2, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "name": "02-dph-model-training.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
