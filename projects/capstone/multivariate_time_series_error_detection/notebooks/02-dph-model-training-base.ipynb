{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.1\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.special import expit\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "def split_companies_train_dev_test(companies):\n",
    "    \"Return train, dev, test set for companies\"\n",
    "    train, test = train_test_split(companies, test_size=0.1, stratify = companies.sector)\n",
    "    train, dev = train_test_split(train, test_size=0.1, stratify = train.sector)\n",
    "    return train, dev, test\n",
    "\n",
    "\n",
    "def filter_stocks(stocks, tickers):\n",
    "    return stocks.loc[tickers]\n",
    "\n",
    "\n",
    "def df_to_ts(df):\n",
    "    res = df.copy()\n",
    "    res.index = pd.DatetimeIndex(pd.to_datetime(res.date))\n",
    "    res.drop('date', axis=1)\n",
    "    return res\n",
    "\n",
    "\n",
    "def log_softmax(x):\n",
    "    return x - np.log(np.sum(np.exp(x)))\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return expit(x)\n",
    "\n",
    "\n",
    "def sample_correlation(df, window_size=63):\n",
    "    idx = np.random.randint(0, df.shape[0]-window_size)\n",
    "    ts = df[idx:idx+window_size]\n",
    "    fmap = lambda s: ts['pct_return'].corr(ts[s])\n",
    "    indices = ts.columns.tolist()[1:]\n",
    "    correlations = np.array(list(map(fmap, indices)))\n",
    "    return correlations\n",
    "\n",
    "\n",
    "def create_correlation_score(df, sample_size=1):\n",
    "    res = np.array([log_softmax(sample_correlation(df)/0.05)\n",
    "                    for i in range(sample_size)])\n",
    "    return np.exp(np.nanmean(res, 0))\n",
    "\n",
    "\n",
    "def load_data(stock_filename=None, indices_filename=None):\n",
    "\n",
    "    if stock_filename is None:\n",
    "        stock_filename = '../../data/processed/wiki_stocks_returns.csv'\n",
    "\n",
    "    if indices_filename is None:\n",
    "        indices_filename = '../../data/processed/wiki_indices_returns.csv'\n",
    "\n",
    "    stocks = pd.read_csv(stock_filename, index_col=False) # long format\n",
    "    indices = pd.read_csv(indices_filename, index_col=False) # wide format\n",
    "\n",
    "    # Implementation of hierarchical clustering\n",
    "    drop_column = lambda df,i=0: df.drop(df.columns[i], axis=1)\n",
    "\n",
    "    stocks = drop_column(stocks)\n",
    "    stocks = stocks.drop('name', axis=1)\n",
    "    stocks = stocks.dropna()\n",
    "\n",
    "    companies = stocks.groupby('ticker').first().reset_index()\n",
    "    sectors_counts = companies.sector.value_counts()\n",
    "    sectors_proportions = sectors_counts/sectors_counts.sum()\n",
    "    sectors_unique = sectors_counts.index.tolist()\n",
    "\n",
    "    stocks = stocks.set_index('ticker')\n",
    "\n",
    "    indices_ts = df_to_ts(indices[['date'] + sectors_unique])\n",
    "    stocks_ts = df_to_ts(stocks.reset_index())\n",
    "\n",
    "    stocks_all = pd.merge(stocks_ts, indices_ts, 'left')\n",
    "    stocks_all = stocks_all.dropna() # loss of 200 000 observations\n",
    "    stocks_all = stocks_all.drop('sector', axis=1)\n",
    "    stocks_all = stocks_all.groupby('ticker').apply(df_to_ts)\n",
    "    stocks_all = stocks_all.drop(['ticker', 'date'], axis=1)\n",
    "    stocks_all = stocks_all.rename(columns={'close': 'pct_return'})\n",
    "\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    label_encoder.fit(sectors_counts.index.tolist())\n",
    "    ticker_to_sector = dict(zip(companies.ticker, label_encoder.transform(companies.sector)))\n",
    "\n",
    "    return stocks_all, companies, label_encoder, ticker_to_sector\n",
    "\n",
    "def sectors_statistics(companies):\n",
    "    sectors_counts = companies.sector.value_counts()\n",
    "    sectors_proportions = sectors_counts/sectors_counts.sum()\n",
    "    sectors_unique = sectors_counts.index.tolist()\n",
    "    return sectors_counts, sectors_proportions, sectors_unique\n",
    "\n",
    "\n",
    "def random_subset(df, window_size=21):\n",
    "    idx = np.random.randint(0, df.shape[0]-window_size)\n",
    "    ts = df[idx:idx+window_size]\n",
    "    return ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most representated class: Financial Services , with proportion of  13.09 %.\n"
     ]
    }
   ],
   "source": [
    "# Make train dev test set.\n",
    "np.random.seed(42)\n",
    "\n",
    "### Feature engineering\n",
    "\n",
    "stock_filename = '../data/processed/wiki_stocks_returns.csv'\n",
    "indices_filename = '../data/processed/wiki_indices_returns.csv'\n",
    "\n",
    "stocks_all, companies, label_encoder, ticker_to_sector = load_data(stock_filename, indices_filename)\n",
    "sectors_counts, sectors_proportions, sectors_unique = sectors_statistics(companies)\n",
    "\n",
    "max_proportion_baseline = sectors_proportions.max()\n",
    "biggest_sector = sectors_proportions.argmax()\n",
    "\n",
    "print(\"Most representated class:\", biggest_sector, ', with proportion of ', round(100*max_proportion_baseline, 2), '%.')\n",
    "# Accuracy of our models should be better than max_proportion_baseline.\n",
    "\n",
    "companies_data = {}\n",
    "data_split = split_companies_train_dev_test(companies)\n",
    "for i, k in enumerate(['train', 'dev', 'test']):\n",
    "    companies_data[k] = data_split[i]\n",
    "stocks_data = {k: filter_stocks(stocks_all, v.ticker) for k, v in companies_data.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample  0\n",
      "Sample  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:53: RuntimeWarning: Mean of empty slice\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using sample correlation, we have the followin accuracies:\n",
      " dev      0.589394\n",
      "test     0.591781\n",
      "train    0.589370\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "### Correlation scores\n",
    "\n",
    "def fmap(companie_ticker, dataset):\n",
    "    return create_correlation_score(dataset.loc[companie_ticker])\n",
    "\n",
    "def base_model():\n",
    "\n",
    "    if True:\n",
    "        accuracies = {}.fromkeys(['train', 'dev', 'test'], 0)\n",
    "        accuracies_df = pd.Series(accuracies)\n",
    "        scores_prediction = dict(accuracies)\n",
    "        predictions = {}\n",
    "        n_sample = 10\n",
    "        for i in range(n_sample):\n",
    "            if i % 5 == 0:\n",
    "                print('Sample ', i)\n",
    "            for dataset_type in ['train', 'dev', 'test']:\n",
    "                tickers = companies_data[dataset_type].ticker.tolist()\n",
    "                fmap_partial = functools.partial(fmap, dataset=stocks_data[dataset_type])\n",
    "                scores = dict(zip(tickers, map(fmap_partial, tickers)))\n",
    "                scores = pd.DataFrame(scores, index=stocks_all.columns[1:])\n",
    "                scores = scores.dropna(axis=1)\n",
    "                y_pred = label_encoder.transform(scores.apply(np.argmax))\n",
    "                y_true = np.array([ticker_to_sector[k] for k in scores.columns])\n",
    "                accuracies[dataset_type] = accuracy_score(y_true, y_pred)\n",
    "                scores_prediction[dataset_type] += scores\n",
    "                # predictions[dataset_type] = y_pred\n",
    "            accuracies = pd.Series(accuracies)\n",
    "            accuracies_df += accuracies\n",
    "        print(\"Using sample correlation, we have the followin accuracies:\\n\", accuracies_df/n_sample)\n",
    "        # dev      0.581667\n",
    "        # test     0.607123\n",
    "        # train    0.587883\n",
    "    return accuracies_df/n_sample\n",
    "\n",
    "accuracies_base = base_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "name": "02-dph-model-training.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
