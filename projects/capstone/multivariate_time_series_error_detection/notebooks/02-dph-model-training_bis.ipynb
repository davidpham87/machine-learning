{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.special import expit\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "def split_companies_train_dev_test(companies):\n",
    "    \"Return train, dev, test set for companies\"\n",
    "    train, test = train_test_split(companies, test_size=0.1, stratify = companies.sector)\n",
    "    train, dev = train_test_split(train, test_size=0.1, stratify = train.sector)\n",
    "    return train, dev, test\n",
    "\n",
    "\n",
    "def filter_stocks(stocks, tickers):\n",
    "    return stocks.loc[tickers]\n",
    "\n",
    "\n",
    "def df_to_ts(df):\n",
    "    res = df.copy()\n",
    "    res.index = pd.DatetimeIndex(pd.to_datetime(res.date))\n",
    "    res.drop('date', axis=1)\n",
    "    return res\n",
    "\n",
    "\n",
    "def log_softmax(x):\n",
    "    return x - np.log(np.sum(np.exp(x)))\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return expit(x)\n",
    "\n",
    "\n",
    "def sample_correlation(df, window_size=63):\n",
    "    idx = np.random.randint(0, df.shape[0]-window_size)\n",
    "    ts = df[idx:idx+window_size]\n",
    "    fmap = lambda s: ts['pct_return'].corr(ts[s])\n",
    "    indices = ts.columns.tolist()[1:]\n",
    "    correlations = np.array(list(map(fmap, indices)))\n",
    "    return correlations\n",
    "\n",
    "\n",
    "def create_correlation_score(df, sample_size=1):\n",
    "    res = np.array([log_softmax(sample_correlation(df)/0.05)\n",
    "                    for i in range(sample_size)])\n",
    "    return np.exp(np.nanmean(res, 0))\n",
    "\n",
    "\n",
    "def load_data(stock_filename=None, indices_filename=None):\n",
    "\n",
    "    if stock_filename is None:\n",
    "        stock_filename = '../../data/processed/wiki_stocks_returns.csv'\n",
    "\n",
    "    if indices_filename is None:\n",
    "        indices_filename = '../../data/processed/wiki_indices_returns.csv'\n",
    "\n",
    "    stocks = pd.read_csv(stock_filename, index_col=False) # long format\n",
    "    indices = pd.read_csv(indices_filename, index_col=False) # wide format\n",
    "\n",
    "    # Implementation of hierarchical clustering\n",
    "    drop_column = lambda df,i=0: df.drop(df.columns[i], axis=1)\n",
    "\n",
    "    stocks = drop_column(stocks)\n",
    "    stocks = stocks.drop('name', axis=1)\n",
    "    stocks = stocks.dropna()\n",
    "\n",
    "    companies = stocks.groupby('ticker').first().reset_index()\n",
    "    sectors_counts = companies.sector.value_counts()\n",
    "    sectors_proportions = sectors_counts/sectors_counts.sum()\n",
    "    sectors_unique = sectors_counts.index.tolist()\n",
    "\n",
    "    stocks = stocks.set_index('ticker')\n",
    "\n",
    "    indices_ts = df_to_ts(indices[['date'] + sectors_unique])\n",
    "    stocks_ts = df_to_ts(stocks.reset_index())\n",
    "\n",
    "    stocks_all = pd.merge(stocks_ts, indices_ts, 'left')\n",
    "    stocks_all = stocks_all.dropna() # loss of 200 000 observations\n",
    "    stocks_all = stocks_all.drop('sector', axis=1)\n",
    "    stocks_all = stocks_all.groupby('ticker').apply(df_to_ts)\n",
    "    stocks_all = stocks_all.drop(['ticker', 'date'], axis=1)\n",
    "    stocks_all = stocks_all.rename(columns={'close': 'pct_return'})\n",
    "\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    label_encoder.fit(sectors_counts.index.tolist())\n",
    "    ticker_to_sector = dict(zip(companies.ticker, label_encoder.transform(companies.sector)))\n",
    "\n",
    "    return stocks_all, companies, label_encoder, ticker_to_sector\n",
    "\n",
    "def sectors_statistics(companies):\n",
    "    sectors_counts = companies.sector.value_counts()\n",
    "    sectors_proportions = sectors_counts/sectors_counts.sum()\n",
    "    sectors_unique = sectors_counts.index.tolist()\n",
    "    return sectors_counts, sectors_proportions, sectors_unique\n",
    "\n",
    "\n",
    "def add_common_layers(y):\n",
    "    y = keras.layers.BatchNormalization()(y)\n",
    "    y = keras.layers.LeakyReLU()(y)\n",
    "    return y\n",
    "\n",
    "\n",
    "def grouped_convolution(y, nb_channels, _strides, cardinality=4):\n",
    "    # when `cardinality` == 1 this is just a standard convolution\n",
    "    if cardinality == 1:\n",
    "        return keras.layers.Conv1D(nb_channels, kernel_size=10, strides=_strides, padding='same')(y)\n",
    "\n",
    "    assert not nb_channels % cardinality\n",
    "    _d = nb_channels // cardinality\n",
    "\n",
    "    # in a grouped convolution layer, input and output channels are divided into `cardinality` groups,\n",
    "    # and convolutions are separately performed within each group\n",
    "    groups = []\n",
    "    for j in range(cardinality):\n",
    "        group = keras.layers.Lambda(lambda z: z[:, :, j * _d:j * _d + _d])(y)\n",
    "        groups.append(keras.layers.Conv1D(_d, kernel_size=10, strides=_strides, padding='same')(group))\n",
    "\n",
    "    # the grouped convolutional layer concatenates them as the outputs of the layer\n",
    "    y = keras.layers.concatenate(groups)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def residual_block(y, nb_channels_in, nb_channels_out, cardinality=4, _strides=1, _project_shortcut=False):\n",
    "    \"\"\"\n",
    "    Our network consists of a stack of residual blocks. These blocks have the same topology,\n",
    "    and are subject to two simple rules:\n",
    "    - If producing spatial maps of the same size, the blocks share the same hyper-parameters (width and filter sizes).\n",
    "    - Each time the spatial map is down-sampled by a factor of 2, the width of the blocks is multiplied by a factor of 2.\n",
    "    \"\"\"\n",
    "    shortcut = y\n",
    "    kl = keras.layers\n",
    "    # we modify the residual building block as a bottleneck design to make the network more economical\n",
    "    y = kl.Conv1D(nb_channels_in, kernel_size=1, strides=1, padding='same')(y)\n",
    "    y = add_common_layers(y)\n",
    "\n",
    "    # ResNeXt (identical to ResNet when `cardinality` == 1)\n",
    "    y = grouped_convolution(y, nb_channels_in, _strides=_strides)\n",
    "    y = add_common_layers(y)\n",
    "\n",
    "    y = kl.Conv1D(nb_channels_out, kernel_size=1, strides=1, padding='same')(y)\n",
    "    # batch normalization is employed after aggregating the transformations and before adding to the shortcut\n",
    "    y = kl.BatchNormalization()(y)\n",
    "\n",
    "    # identity shortcuts used directly when the input and output are of the same dimensions\n",
    "    if _project_shortcut or _strides != 1:\n",
    "        # when the dimensions increase projection shortcut is used to match dimensions (done by 1Ã—1 convolutions)\n",
    "        # when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2\n",
    "        shortcut = kl.Conv1D(nb_channels_out, kernel_size=1, strides=_strides, padding='same')(shortcut)\n",
    "        shortcut = kl.BatchNormalization()(shortcut)\n",
    "\n",
    "    y = kl.add([shortcut, y])\n",
    "\n",
    "    # relu is performed right after each batch normalization,\n",
    "    # expect for the output of the block where relu is performed after the adding to the shortcut\n",
    "    y = kl.LeakyReLU()(y)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "# reparameterization trick\n",
    "# instead of sampling from Q(z|X), sample eps = N(0,I)\n",
    "# z = z_mean + sqrt(var)*eps\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "    # Arguments:\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns:\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "    z_mean, z_log_var = args\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    dim = keras.backend.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = tf.random_normal(shape=(batch, dim))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "class CovarianceLayer(keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        self.num_classes = num_classes\n",
    "        super(CovarianceLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        super(CovarianceLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs):\n",
    "        series_input, environment_input = inputs\n",
    "        series_input_multiple = tf.tile(series_input, [1, 1, self.num_classes])\n",
    "        covariances = tf.reduce_mean(series_input_multiple * environment_input, axis=1);\n",
    "        return covariances\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "\n",
    "def random_subset(df, window_size=21):\n",
    "    idx = np.random.randint(0, df.shape[0]-window_size)\n",
    "    ts = df[idx:idx+window_size]\n",
    "    return ts\n",
    "\n",
    "\n",
    "def make_keras_subset(dataset_type, companies_data, stocks_data, label_encoder, batch_size, window_size=21):\n",
    "    idx = np.random.choice(companies_data[dataset_type].shape[0], batch_size)\n",
    "    df = companies_data[dataset_type].iloc[idx]\n",
    "\n",
    "    model_input_data = [random_subset(stocks_data[dataset_type].loc[t], window_size) for t in df.ticker]\n",
    "    model_series_input = np.array([df['pct_return'].values for df in model_input_data])\n",
    "    model_series_input = model_series_input.reshape(-1, window_size, 1)\n",
    "\n",
    "    model_environment_input = np.array([df.iloc[:, 1:].values for df in model_input_data])\n",
    "\n",
    "    y_true = label_encoder.transform(df.sector)\n",
    "\n",
    "    return model_series_input, model_environment_input, y_true\n",
    "\n",
    "\n",
    "class StocksSequence(keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, stocks_data,  companies_data, window_size, label_encoder, batch_size):\n",
    "        self.stocks_data = stocks_data\n",
    "        self.batch_size = batch_size\n",
    "        self.label_encoder = label_encoder\n",
    "        self.companies_data = companies_data\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.stocks_data.shape[0] / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        idx = np.random.choice(self.companies_data.shape[0], self.batch_size)\n",
    "        df = self.companies_data.iloc[idx]\n",
    "        model_input_data = [random_subset(self.stocks_data.loc[t], self.window_size) for t in df.ticker]\n",
    "        model_series_input = np.array([df['pct_return'].values for df in model_input_data])\n",
    "        model_series_input = model_series_input.reshape(-1, self.window_size, 1)\n",
    "        model_environment_input = np.array([df.iloc[:, 1:].values for df in model_input_data])\n",
    "        y_true = self.label_encoder.transform(df.sector)\n",
    "\n",
    "        return [model_series_input, model_environment_input], y_true\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "?keras.layers.Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_model(num_classes=16, window_size=21, latent_dim=32):\n",
    "    kl = keras.layers\n",
    "\n",
    "    series_input = keras.layers.Input(shape=(window_size, 1), dtype='float32', name='series_input')\n",
    "    environment_input = kl.Input(shape=(window_size, num_classes), dtype='float32', name='environment_input')\n",
    "    x = kl.Concatenate()([series_input, environment_input])\n",
    "    tanh_layer = kl.Lambda(lambda y: tf.tanh(tf.scalar_mul(5, y)))\n",
    "    x_tanh = tanh_layer(x)\n",
    "    x = kl.Conv1D(16, 5, 1, activation='relu')(x_tanh)\n",
    "    x = kl.Dropout(0.01)(x)\n",
    "    x = residual_block(x, 16, 16, 2)\n",
    "    x = kl.MaxPool1D()(x)\n",
    "    x = kl.Conv1D(32, 21, 1, activation='relu')(x)\n",
    "    x = kl.Dropout(0.01)(x)\n",
    "    x = residual_block(x, 32, 32, 2)\n",
    "    x = kl.MaxPool1D()(x)\n",
    "    \n",
    "    \n",
    "    covariances = CovarianceLayer(num_classes)([tanh_layer(series_input), tanh_layer(environment_input)])\n",
    "    covariances = kl.Dense(16, activation='relu')(covariances)\n",
    "    covariances = kl.Dense(32, activation='relu')(covariances)\n",
    "\n",
    "    x = kl.Flatten()(x)\n",
    "    x = kl.Dense(32, 'relu', kernel_regularizer=keras.regularizers.l2(0.01))(x)\n",
    "    x = kl.Dropout(0.01)(x)\n",
    "    z = kl.Multiply(name='Embedding')([x, covariances])\n",
    "    z = kl.Dense(32, 'relu', kernel_regularizer=keras.regularizers.l2(0.01))(z)\n",
    "    x_pred = kl.Dense(num_classes, 'softmax')(z)\n",
    "\n",
    "    model = keras.Model(inputs = [series_input, environment_input], outputs=[x_pred], name='Classifier')\n",
    "\n",
    "    # kl_batch = - .5 * tf.reduce_sum(1 + x_log_var - tf.square(x_mu) - tf.exp(x_log_var), axis=-1)\n",
    "    # model.add_loss(kl_batch)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most representated class: Financial Services , with proportion of  13.09 %.\n"
     ]
    }
   ],
   "source": [
    "# Make train dev test set.\n",
    "np.random.seed(42)\n",
    "\n",
    "### Feature engineering\n",
    "\n",
    "stock_filename = '../data/processed/wiki_stocks_returns.csv'\n",
    "indices_filename = '../data/processed/wiki_indices_returns.csv'\n",
    "\n",
    "stocks_all, companies, label_encoder, ticker_to_sector = load_data(stock_filename, indices_filename)\n",
    "sectors_counts, sectors_proportions, sectors_unique = sectors_statistics(companies)\n",
    "\n",
    "max_proportion_baseline = sectors_proportions.max()\n",
    "biggest_sector = sectors_proportions.argmax()\n",
    "\n",
    "print(\"Most representated class:\", biggest_sector, ', with proportion of ', round(100*max_proportion_baseline, 2), '%.')\n",
    "# Accuracy of our models should be better than max_proportion_baseline.\n",
    "\n",
    "companies_data = {}\n",
    "data_split = split_companies_train_dev_test(companies)\n",
    "for i, k in enumerate(['train', 'dev', 'test']):\n",
    "    companies_data[k] = data_split[i]\n",
    "stocks_data = {k: filter_stocks(stocks_all, v.ticker) for k, v in companies_data.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/backend.py:1456: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "window_size = 126\n",
    "model = make_model(window_size=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model.load_weights('checkpoint/model_weights_second.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def schedule_fn(epoch):\n",
    "   initial_lrate = 0.001\n",
    "   drop = 0.5\n",
    "   epochs_drop = 20.0\n",
    "   lrate = initial_lrate * math.pow(drop,  \n",
    "           math.floor((1+epoch)/epochs_drop))\n",
    "   return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/backend.py:1557: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/backend.py:1422: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "series_input (InputLayer)       (None, 126, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "environment_input (InputLayer)  (None, 126, 16)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 126, 17)      0           series_input[0][0]               \n",
      "                                                                 environment_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               multiple             0           concatenate_1[0][0]              \n",
      "                                                                 series_input[0][0]               \n",
      "                                                                 environment_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 122, 16)      1376        lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 122, 16)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 122, 16)      272         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 122, 16)      64          conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 122, 16)      0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 122, 4)       0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 122, 4)       0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 122, 4)       0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 122, 4)       0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 122, 4)       164         lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 122, 4)       164         lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 122, 4)       164         lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 122, 4)       164         lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 122, 16)      0           conv1d_3[0][0]                   \n",
      "                                                                 conv1d_4[0][0]                   \n",
      "                                                                 conv1d_5[0][0]                   \n",
      "                                                                 conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 122, 16)      64          concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 122, 16)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 122, 16)      272         leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 122, 16)      64          conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 122, 16)      0           dropout_1[0][0]                  \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 122, 16)      0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 61, 16)       0           leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 41, 32)       10784       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 41, 32)       0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 41, 32)       1056        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 41, 32)       128         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 41, 32)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 41, 8)        0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 41, 8)        0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 41, 8)        0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 41, 8)        0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 41, 8)        648         lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 41, 8)        648         lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 41, 8)        648         lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 41, 8)        648         lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 41, 32)       0           conv1d_10[0][0]                  \n",
      "                                                                 conv1d_11[0][0]                  \n",
      "                                                                 conv1d_12[0][0]                  \n",
      "                                                                 conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 41, 32)       128         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 41, 32)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 41, 32)       1056        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 41, 32)       128         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 41, 32)       0           dropout_2[0][0]                  \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 41, 32)       0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 20, 32)       0           leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 640)          0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "covariance_layer_1 (CovarianceL (None, 16)           0           lambda_1[1][0]                   \n",
      "                                                                 lambda_1[2][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 32)           20512       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           272         covariance_layer_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           544         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding (Multiply)            (None, 32)           0           dropout_3[0][0]                  \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_4 (Dense)                 (None, 32)           1056        Embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           528         dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 41,552\n",
      "Trainable params: 41,264\n",
      "Non-trainable params: 288\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "optimizer = keras.optimizers.Adam(0.001)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint('checkpoint/model_weights_eighth.json', monitor='val_acc', verbose=1, save_best_only=True, mode='max'),\n",
    "    keras.callbacks.TensorBoard(log_dir='./logs/eighth'),\n",
    "    keras.callbacks.LearningRateScheduler(schedule_fn)\n",
    "]\n",
    "\n",
    "stocks_sequence_training = StocksSequence(stocks_data['train'], companies_data['train'], window_size, label_encoder, batch_size)\n",
    "stocks_sequence_validation = StocksSequence(stocks_data['dev'], companies_data['dev'], window_size, label_encoder, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1999/2000 [============================>.]1999/2000 [============================>.] - ETA: 0s - loss: 2.3779 - acc: 0.2512Epoch 00001: val_acc improved from -inf to 0.22219, saving model to checkpoint/model_weights_eighth.json\n",
      "2000/2000 [==============================]2000/2000 [==============================] - 225s 112ms/step - loss: 2.3777 - acc: 0.2512 - val_loss: 2.8392 - val_acc: 0.2222\n",
      "\n",
      "Epoch 2/500\n",
      "1999/2000 [============================>.]1999/2000 [============================>.] - ETA: 0s - loss: 2.0546 - acc: 0.3524Epoch 00002: val_acc did not improve\n",
      "2000/2000 [==============================]2000/2000 [==============================] - 171s 85ms/step - loss: 2.0545 - acc: 0.3524 - val_loss: 2.8178 - val_acc: 0.2013\n",
      "\n",
      "Epoch 3/500\n",
      "1999/2000 [============================>.]1999/2000 [============================>.] - ETA: 0s - loss: 1.9416 - acc: 0.3948Epoch 00003: val_acc improved from 0.22219 to 0.29781, saving model to checkpoint/model_weights_eighth.json\n",
      "2000/2000 [==============================]2000/2000 [==============================] - 204s 102ms/step - loss: 1.9415 - acc: 0.3948 - val_loss: 2.6065 - val_acc: 0.2978\n",
      "\n",
      "Epoch 4/500\n",
      "1999/2000 [============================>.]1999/2000 [============================>.] - ETA: 0s - loss: 1.8161 - acc: 0.4269Epoch 00004: val_acc improved from 0.29781 to 0.35078, saving model to checkpoint/model_weights_eighth.json\n",
      "2000/2000 [==============================]2000/2000 [==============================] - 254s 127ms/step - loss: 1.8161 - acc: 0.4269 - val_loss: 2.1902 - val_acc: 0.3508\n",
      "\n",
      "Epoch 5/500\n",
      "1999/2000 [============================>.]1999/2000 [============================>.] - ETA: 0s - loss: 1.7523 - acc: 0.4468Epoch 00005: val_acc did not improve\n",
      "2000/2000 [==============================]2000/2000 [==============================] - 210s 105ms/step - loss: 1.7522 - acc: 0.4469 - val_loss: 2.8784 - val_acc: 0.2687\n",
      "\n",
      "Epoch 6/500\n",
      "1937/2000 [============================>.]1937/2000 [============================>.] - ETA: 6s - loss: 1.7197 - acc: 0.4570"
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    stocks_sequence_training, steps_per_epoch=2000, epochs=500, callbacks=callbacks,\n",
    "    validation_data = stocks_sequence_validation, validation_steps=100, workers=4, max_queue_size=20, verbose=1,\n",
    "    use_multiprocessing=True)\n",
    "\n",
    "stocks_sequence_test = StocksSequence(stocks_data['test'], companies_data['test'], window_size, label_encoder, batch_size)\n",
    "\n",
    "model.evaluate_generator(stocks_sequence_test, 200, workers=2, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "name": "02-dph-model-training.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
